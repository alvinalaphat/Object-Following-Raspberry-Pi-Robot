The project will require color detection, depth detection, and shape detection for the computer vision algorithm. Whatever object it’s following will have a distinct color from its background, will be maintained in closer proximity to the chassis of the vehicle, and will have a distinct shape so the algorithm can distinguish it from its surroundings. The object will have to be identified accurately and consistently for the robot to know to follow it and to recognize it as fast as possible and react. The algorithm will likely be dependent on consistent exterior lighting in the room. It also will likely require no other objects to distract it from its image processing to ensure the fastest response times. The classifier will be a standard CNN that detects tennis balls for example but we will want to take into consideration the hand that’s holding the ball in the video. The camera will have to be mounted on a swivel so it can react and adjust its angle to the ball in real time, followed by a response to the motors to push the car in that direction where the exact turn radius will have to be calculated. The power applied to the wheels will also have to be adjusted based on the speed at which the distance is varying between the ball and the camera lens. Shutter speed can enhance the algorithm's response times as well. 

On the hardware side, the project will require a raspberry pi, a car chassis, motors, motor controllers, wiring, and batteries. We haven’t worked with hardware before so this will be one of the biggest challenges. The motors of the car will have to work fast in response to the movement of the object that is to be tracked so it can keep the object in the center of the frame at all times when it follows it straight on. The turn radius of the car will have to be measured so we can account for it in the algorithm of how many degrees to turn the car. 

The training data will come from images online that are compiled to understand the shape and color of the tennis ball we are going to be detecting. Because we are considering the hand holding the ball in the video, we might want to consider also training images that we manually take of our hand holding the ball so we can train the model in the exact environment it’s going to be tested in. The testing data can be videos of our hand holding the ball so we can see if the camera can follow and react to the movement of the ball. 
